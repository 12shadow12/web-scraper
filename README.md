# Wikipedia Web Scraper

This project is a Python-based web scraper designed to extract data from Wikipedia pages automatically. It demonstrates how to collect, parse, and save structured information from Wikipedia for analysis or research purposes.

---

## Features

- Scrapes specified Wikipedia pages to collect data  
- Parses HTML content using BeautifulSoup  
- Extracts tables, text, or other structured data from Wikipedia articles  
- Saves scraped data to CSV or JSON files  
- Easily configurable for different Wikipedia pages or data types  

---

## Technologies Used

- Python 3  
- Requests  
- BeautifulSoup  
- CSV / JSON libraries  

---

## Getting Started

### Prerequisites

- Python 3.x installed on your machine  
- (Optional) Virtual environment recommended  

### Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/12shadow12/web-scraper.git
   cd web-scraper
